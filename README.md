# senior_thesis_code
All code used for senior thesis 


-- scraper.py -- contains the main code used to scrape the dataset from New York Times

-- cleandata.py -- contains the code used to create the "cleaned" and "processed" versions of the dataset

-- sentiment_analyzer.py -- contains the code used to conduct sentiment analysis experiments

-- make_embeddings.py -- contains both the code used to train the word embeddings and also perform analysis on them

-- nyt_developer_api.py -- contains various helper functions I used to help me understand how to use the NYT developer API



All datasets used for senior thesis 

-- links_only -- contains links used for scraping
-- data -- contains raw data from all scraped articles 
-- cleaned -- contains "cleaned" dataset (detailed in paper)
-- processed -- contains "processed" dataset (detailed in paper) 
